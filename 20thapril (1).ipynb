{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef52230-2099-4d2a-a8b8-0ae5a272be2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1:\n",
    "  KNN (K-Nearest Neighbors) is a machine learning algorithm used for both classification and regression \n",
    "problems. The algorithm works by finding the K closest data points in the training set to a new data point\n",
    "and then predicting the class or value based on the majority class or average value of the K neighbors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63088a37-14cd-41c4-b1a1-b1fabe4456cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc0443-6db8-447c-b117-afff0fe37f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "2:\n",
    "  The value of K in KNN is a hyperparameter that needs to be tuned to achieve the best performance.\n",
    "One common approach is to use cross-validation to evaluate the performance of the model for different\n",
    "values of K and select the K value that gives the best performance. Another approach is to use domain\n",
    "knowledge or intuition to select an appropriate K value based on the characteristics of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1fa83b-709d-4af3-9c7c-b86031596c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbb3181-2a88-4f44-add4-4c1358e5b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3:\n",
    " KNN classifier is used for classification problems where the goal is to predict a categorical output\n",
    "variable, while KNN regressor is used for regression problems where the goal is to predict a continuous\n",
    "output variable. In KNN classifier, the output is the class label of the majority of the K nearest neighbors,\n",
    "while in KNN regressor, the output is the average of the K nearest neighbors.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a1c4f-9e47-4207-8f7b-a0b6d1930b6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e567a08-3a7c-4e83-9df9-d6a259af1ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "4:\n",
    "  The performance of KNN can be measured using various metrics depending on the type of problem.\n",
    "For classification problems, common metrics include accuracy, precision, recall, F1-score, and \n",
    "ROC curve. For regression problems, common metrics include mean squared error (MSE), mean absolute error (MAE),\n",
    "and R-squared. Cross-validation can also be used to evaluate the performance of KNN and tune the hyperparameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0ab5bc-77a2-4768-a9fa-4c3bbcdaf0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aec6358-d778-42f6-8b22-64a71531f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "5:\n",
    " The curse of dimensionality refers to the problem of increasing feature dimensions in a dataset, \n",
    "which causes the distance between points to become less meaningful. In KNN, as the number of dimensions\n",
    "(features) in the dataset increases, the distance between points becomes less meaningful, and the algorithm \n",
    "starts to perform poorly. This is because the algorithm relies on proximity between points to classify new \n",
    "data points, and as the number of dimensions increases, the number of points required to maintain the same\n",
    "level of proximity increases exponentially.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57692e-154b-4110-9245-9a9980ed1423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940124b-fa7e-4e73-8d06-23757454b4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "6:\n",
    " KNN is sensitive to missing values because it relies on the distance between data points to make predictions. There are several ways to handle missing values in KNN:\n",
    "1.\n",
    "'Deletion': One approach is to simply delete any data points that have missing values. However, this can result in a loss of information and may not be feasible if there are many missing values.\n",
    "2.\n",
    "'Imputation': Another approach is to impute missing values by replacing them with estimated values. One common method is to replace missing values with the mean or median of the feature values for the other data points in the same class.\n",
    "3.\n",
    "'KNN Imputation': A more sophisticated approach is to use KNN to impute missing values. This involves using KNN to identify the k-nearest neighbors to the data point with missing values, and then using the feature values of those neighbors\n",
    "to estimate the missing values. This method can be more accurate than simple imputation methods but can be computationally expensive.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9dcea-2cf5-4c3c-a9f5-ab0563f9120a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013d5eee-9722-457b-8392-82163f00aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "7:\n",
    " KNN (K-Nearest Neighbors) is a supervised learning algorithm that can be used for both classification\n",
    "and regression problems. The difference between KNN classifier and regressor is that the former is used \n",
    "for classification problems, where we predict a categorical variable, while the latter is used for\n",
    "regression problems, where we predict a continuous variable.\n",
    "In terms of performance, KNN classifier is generally better for problems with a smaller number of classes\n",
    "and a larger number of samples. It works well when the decision boundary between classes is clearly defined\n",
    "and the data is not too noisy. However, it may not perform as well when the data is highly imbalanced or \n",
    "when the feature space is high-dimensional.\n",
    "On the other hand, KNN regressor is better suited for problems where the target variable is continuous \n",
    "and the relationship between the features and the target variable is non-linear. It can handle noisy data\n",
    "and is robust to outliers. However, it may not perform well when the feature space is high-dimensional \n",
    "or when the data is sparse.\n",
    "In summary, the choice between KNN classifier and regressor depends on the nature of the problem at hand.\n",
    "If the problem involves predicting a categorical variable with a small number of classes, KNN classifier\n",
    "may be a better choice. If the problem involves predicting a continuous variable with a non-linear\n",
    "relationship between features and target, KNN regressor may be a better choice.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15517489-5df8-4e89-a145-9176f87430dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9216ef-65d3-48f3-9a33-44a553e3eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "8:\n",
    " The KNN algorithms strengths include its simplicity, easy interpretability, and effectiveness in handling\n",
    "both classification and regression tasks. However, its weaknesses include its sensitivity to outliers, \n",
    "lack of scalability, and high computational cost during prediction. To address these issues, we can use\n",
    "techniques such as outlier detection and removal, dimensionality reduction, and data sampling to improve\n",
    "the algorithm's performance.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d0916-9575-48a8-ad48-0df2da23669c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6292f1bf-9c5d-4927-89fa-3fca6c5a09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "9:\n",
    "  The difference between Euclidean distance and Manhattan distance in KNN lies in how they measure \n",
    "the distance between two points. Euclidean distance calculates the shortest distance between two points\n",
    "in a straight line, whereas Manhattan distance calculates the distance between two points by summing the\n",
    "absolute differences of their coordinates. In simpler terms, Euclidean distance is the length of a straight\n",
    "line between two points, while Manhattan distance is the distance between two points measured along the axes at right angles.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b196cb4-db7e-44cd-b62b-9e880fcccdb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c7fbff-3ea5-4099-9254-90a329b76e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "10:\n",
    "   Feature scaling in KNN is essential to ensure that all features are on the same scale and have the\n",
    "same weightage during distance calculation. Since KNN uses a distance-based approach to compute the \n",
    "similarity between data points, it is crucial to scale the features to avoid bias towards features \n",
    "with a large scale. Scaling techniques such as normalization and standardization can be used to bring\n",
    "all features to a common scale and improve the performance of KNN.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a9243-27fd-4544-8d77-51de4450176e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
